\chapter{El Método de Separación de Va\-riables} \label{chap:MSV}

Ya observamos que podemos hacer uso del método de la transformada de Fourier para reducir una ecuación diferencial en dos variables diferentes a una EDO en una sola variable. Sin embargo, no siempre será cómodo calcular la transformada de Fourier de una función, por lo que sería agradable tener una forma más general de hacer funcionar esta idea.

Para ello, hacemos uso del \textbf{método de separación de variables}, gracias al cual podemos reducir una EDP lineal de $n$ variables en un conjunto de $n$ EDOs para $n$ funciones auxiliares, cada una asociada a una variable independiente de la EDP \emph{y que no depende de las otras variables independientes}. Antes de ello, introduciremos la siguiente definición.

\begin{defi}\marginnote{Función separable}
    Una función de $n$ variables $u(x_1, x_2, \dots, x_n)$ se dice \textbf{separable} si puede escribirse como producto de dos o más funciones diferentes, donde cada una de ellas depende de variables distintas, es decir,
    \begin{equation}
        u(x_1, x_2, \dots, x_n) = f(x_1, x_2, \dots, x_k) g(x_{k+1}, x_{k+2}, \dots, k_n) \ ,
    \end{equation}
    y se dice \textbf{completamente separable} cuando puede escribirse como el producto de $n$ funciones de una sola variable,
    \begin{equation}
        u(x_1, x_2, \dots, x_n) = X_1(x_1) X_2(x_2) \dots X_n(x_n) \ .
    \end{equation}
\end{defi}

% \begin{defi}\marginnote{Ecuación separable}
%     Diremos que una ecuación diferencial parcial \emph{lineal} es \textbf{separable} si sus soluciones son funciones completamente separables.
% \end{defi} %% En realidad creo que esto no es correcto.

\begin{propo} \marginnote{Método de Separación de Variables}
    \textbf{Método de Separación de Variables} \par

    Dada una EDP lineal de $n$ variables, podemos intentar resolverla mediante el método de separación de variables siguiendo el siguiente algoritmo:
    \begin{enumerate}
        \item Identificamos las variables independientes $x_i$ de nuestra ecuación.
        \item Proponemos una solución (o \emph{ansatz}) completamente separable a nuestra ecuación, correspondiente al producto de $n$ funciones auxiliares, donde cada función depende de solo una variable $x_i$.
        \item Reemplazamos la solución propuesta en el paso anterior en nuestra ecuación diferencial.
        \item Dividimos la ecuación diferencial por la solución separable.
        \item  Reordenamos la ecuación, de modo que a un lado de la igualdad quede una ecuación que depende únicamente de una de las variables.
        \item Dado que ambos lados de la igualdad obtenida en el paso anterior depende de variables diferentes, \emph{separamos las variables}. Con esto nos referimos al hecho que la igualdad será válida únicamente si ambos lados son iguales a una \emph{constante de separación} $\lambda_i$, por lo que igualamos ambas ecuaciones a esta constante.
        \item Hemos reducido nuestra EDP original a un sistema formado por una EDO en la variable $x_i$, y una EDP de $n-1$ variables.
        \item Repetimos el paso 6, hasta haber introducido $n-1$ constantes de separación.
        \item Nuestra EDP se ha convertido en un sistema de $n$ EDOs, las que en principio podemos resolver, hallando las $n$ funciones auxiliares de nuestro ansatz.
        \item Imponemos las restricciones físicas que correspondan a nuestro problema, como puede ser el no incluir soluciones con parte imaginaria. De ser posible, imponemos también las condiciones de borde sobre las EDO, siempre y cuando ellas dependan de una sola variable. 
        \item Hallamos una solución particular a nuestra EDP, correspondiente al producto de las $n$ funciones auxiliares.
        \item La solución general corresponderá a la combinación lineal de todas las posibles soluciones particulares, es decir, a la suma sobre las $n-1$ constantes de separación.
        \item Imponemos las condiciones de borde de nuestro problema.
    \end{enumerate}
\end{propo}

Una duda totalmente razonable es por qué consideramos soluciones separables. La verdad no hay una respuesta directa a esto, más allá de ``\emph{esperemos que funcione}''. Si no fuera el caso, y nuestro sistema comienza a complicarse, puede que sea una mejor idea utilizar algún método alternativo. Sin embargo, cabe mencionar que si los operadores diferenciales (derivadas $n$-ésimas) presentes en nuestra ecuación son aditivos, es decir, no tenemos combinaciones de las variables, una solución de este estilo suele funcionar.



%  Esto lo podemos hacer al proponer que nuestro sistema puede ser descrito mediante una \emph{solución separable}, que consiste en el producto de todas las funciones auxiliares que encontremos mediante la solución de las EDOs.



% Para realizar la separación de las variables, haremos uso de $(n-1)$ \emph{constantes de separación}, las que son escogidas, en primera instancia, de forma arbitraria para luego determinarlas gracias a las condiciones de contorno del sistema.

% Finalmente, una vez hallamos las soluciones de cada una de las EDOs y planteamos la solución separable del sistema, consideraremos que la solución más general consiste en la \emph{superposición} o \emph{combinación lineal} de todas las posibles soluciones separables del sistema.

Es posible que el algoritmo descrito anteriormente no sea tan claro de entender en primera instancia. Por ello, como ejemplo bastante ilustrativo, durante el capítulo resolveremos la ecuación de Helm\-holtz en diferentes sistemas coordenados, dando origen a diferentes \emph{funciones especiales}, que serán discutidas en mayor profundidad en los capítulos siguientes.

\section{Resolviendo la ecuación de Helmholtz}

Recordemos que la ecuación de Helmholtz es dada por la expresión
\begin{equation}\label{eq:Helmholtz}
    \nabla^2 \psi + k^2 \psi = 0 \ ,
\end{equation}
donde $k$ es una constante asociada al sistema. Desarrollaremos esta ecuación en los tres sistemas de coordenadas más comunes en física.

\subsection{Coordenadas cartesianas}

En un sistema cartesiano, el operador laplaciano se define simplemente como
\begin{equation}
    \nabla^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2} \ .
\end{equation}

De esta forma, dado que nuestra ecuación posee tres variables independientes, podremos reducirla a un sistema de 3 EDOs en las variables $x$, $y$ y $z$. Antes de realizar este procedimiento, plantearemos una solución de la forma
\begin{equation}\label{eq:ansatz}
    \psi(x,y,z) = X(x)Y(y)Z(z) \ .
\end{equation}

% Una duda totalmente razonable es por qué considerar una solución de este estilo. La verdad no hay una respuesta directa a esto, más allá de ``\emph{esperemos que funcione}''. Si no fuera el caso, y nuestro sistema comienza a complicarse, puede que sea una mejor idea utilizar algún método alternativo. Sin embargo, cabe mencionar que si los operadores diferenciales (derivadas $n$-ésimas) son aditivos, es decir, no tenemos combinaciones de las variables, una solución de este estilo suele funcionar.

Evaluando la expresión \eqref{eq:ansatz} en la ecuación \eqref{eq:Helmholtz}, podemos escribirla como
\begin{equation}
    YZ \frac{d^2X}{dx^2} + XZ \frac{d^2 Y}{dy^2} + XY \frac{d^2Z}{dz^2} + k^2 XYZ = 0 \ ,
\end{equation}
donde ahora utilizamos derivadas totales en lugar de parciales, puesto que cada una de las funciones depende únicamente de una variable.

Dividimos ahora por la solución $XYZ$, donde hemos asumido que $\psi(x,y,z) \neq 0$, de modo que, luego de reordenar los términos, la ecuación resulta en
\begin{equation} \label{eq:sep_1}
    \frac{1}{X} \frac{d^2 X}{dx^2} = -k^2 - \frac{1}{Y} \frac{d^2 Y}{dy^2} - \frac{1}{Z} \frac{d^2 Z}{dz^2} \ .
\end{equation}

Hemos llegado al paso en donde se aprecia esta \emph{separación de variables}. Notemos que el lado izquierdo de la ecuación \eqref{eq:sep_1} contiene únicamente términos asociados a la variable $x$, mientras que el lado derecho aún tiene dependencia en $y$ y en $z$. La única posibilidad de que ambos lados sean iguales, dado que dependen de variables distintas, es que ambos son a su vez iguales a una \emph{constante de separación}, que en este caso asumiremos real y llamaremos $\lambda_1$,
\begin{align}
    \frac{1}{X} \frac{d^2 X}{dx^2} = \lambda_1 \ , \\
    -k^2 - \frac{1}{Y} \frac{d^2Y}{dy^2} - \frac{1}{Z} \frac{d^2Z}{dz^2} = \lambda_1 \ . \label{eq:EDO_de_y_z}
\end{align}

Notemos que, reordenando términos en la ecuación \eqref{eq:EDO_de_y_z}, podemos nuevamente separar las variables mediante una constante $\lambda_2$, de modo que hemos \emph{dividido la EDP original, dependiente de tres variables, en un sistema de tres EDOs, introduciendo dos constantes de separación},
\begin{align}
    \frac{d^2X}{dx^2} - \lambda_1 X & = 0 \ , \label{eq:EDO_de_x}  \\
    \frac{d^2Y}{dy^2} - \lambda_2 Y & = 0 \ , \label{eq:EDO_de_y}  \\
    \frac{d^2Z}{dz^2} + (k^2 + \lambda_1 + \lambda_2) Z & = 0 \ . \label{eq:EDO_de_z} 
\end{align}

Cada una de estas EDOs son resolubles mediante los métodos vistos en su primer curso de Ecuaciones Diferenciales, y las soluciones dependerán del valor y del signo de las constantes de separación. Analicemos las posibles soluciones de la ecuación \eqref{eq:EDO_de_x}:

\begin{equation}
    X_{\lambda_1}(x) = \left\{
    \begin{array}{cc} 
            c_1 \sinh(\sqrt{\lambda_1} x) + c_2 \cosh(-\sqrt{\lambda_1}x) \ , & \text{si } \lambda_1 > 0 \ , \\
            c_1 + c_2 x \ , & \text{si } \lambda_1 = 0 \ , \\
            c_1 \cos(\sqrt{-\lambda_1}x) + c_2 \sin(\sqrt{-\lambda_1}x) \ , & \text{si } \lambda_1 < 0 \ .
    \end{array}
    \right.
\end{equation}

Aquí es importante no olvidar que tenemos una motivación física para realizar estos cálculos, lo que nos ayudará a determinar el signo de la constante de separación. Para la mayoría de los problemas físicos, la solución para $X(x)$ que tiene sentido es aquella en que $\lambda_1$ es negativo, de modo que la solución es una oscilación en la coordenada $x$. Así mismo, podemos hacer un análisis análogo para cada una de las otras ecuaciones, obteniendo soluciones denotadas por $Y_{\lambda_2}(y)$ y $Z_{\lambda_1 \lambda_2}(z)$, donde es importante indicar el subíndice, ya que esta solución es válida para un valor particular de las constantes de separación.

De esta forma, una solución particular a nuestra EDP es dada por
\begin{equation} \label{eq:sol_particular_cartesianas}
    \psi_{\lambda_1 \lambda_2}(x,y,z) = X_{\lambda_1}(x) Y_{\lambda_2}(y) Z_{\lambda_1 \lambda_2}(z) \ ,
\end{equation}
y la solución general corresponderá a una combinación lineal de la solución \eqref{eq:sol_particular_cartesianas}, correspondiendo a una suma sobre todos los valores posibles de $\lambda_1$ y $\lambda_2$, es decir,
\begin{equation}
    \psi(x,y,z) = \sum_{\lambda_1 \lambda_2} C_{\lambda_1 \lambda_2} \psi_{\lambda_1 \lambda_2}(x,y,z) \ ,
\end{equation}
donde los coeficientes $C_{\lambda_1 \lambda_2}$ serán obtenidos al imponer las condiciones de contorno del problema, que por lo general nos llevará a un conjunto finito de valores para $\lambda_1$ y $\lambda_2$.

% \newpage

\subsection{Coordenadas cilíndricas}

En este caso, el laplaciano debe definirse de una forma diferente, dada por
\begin{equation}
    \nabla^2 = \frac{1}{\rho} \frac{\partial}{\partial \rho} \left( \rho \frac{\partial}{\partial \rho} \right) + \frac{1}{\rho^2} \frac{\partial^2}{\partial \phi^2} + \frac{\partial^2}{\partial z^2} \ , 
\end{equation}
de modo que la ecuación de Helmholtz \eqref{eq:Helmholtz} se podrá escribir como
\begin{equation} \label{eq:Helmholtz_cilindrica}
    \frac{1}{\rho} \frac{\partial}{\partial \rho} \left( \rho \frac{\partial \psi}{\partial \rho} \right) + \frac{1}{\rho^2} \frac{\partial^2 \psi}{\partial \phi^2} + \frac{\partial^2 \psi}{\partial z^2} + k^2 \psi = 0 \ .
\end{equation}

Nuevamente plantearemos una solución que sea producto de tres funciones, cada una dependiente de una de las variables del problema, es decir
\begin{equation}
    \psi(\rho, \phi, z) = P(\rho) \Phi(\phi) Z(z) \ ,
\end{equation}
que al sustituirlas en la expresión \eqref{eq:Helmholtz_cilindrica} resultará en
\begin{equation}
    \frac{\Phi Z}{\rho} \frac{d}{d\rho}\left( \rho \frac{dP}{d\rho} \right) + \frac{PZ}{\rho^2} \frac{d^2 \Phi}{d\phi^2} + P\Phi \frac{d^2Z}{dz^2} + k^2 P\Phi Z = 0 \ ,
\end{equation}
y dividiendo por $P\Phi Z$, junto a un reordenamiento de los términos, llegamos a la expresión
\begin{equation}
    \frac{1}{\rho P} \frac{d}{d\rho} \left( \rho \frac{dP}{d\rho} \right) + \frac{1}{\rho^2 \Phi} \frac{d^2\Phi}{d\phi^2} + k^2 = - \frac{1}{Z} \frac{d^2Z}{dz^2} \ .
\end{equation}

Dado que a ambos lados de la ecuación tenemos una dependencia en variables diferentes, ambos lados deben ser iguales a una constante para no depender de ellas. Llamaremos a esta constante $\lambda_1$, de modo que
\begin{align}
    \frac{1}{Z} \frac{d^2Z}{dz^2} & = - \lambda_1 \ , \\
    \frac{1}{\rho P} \frac{d}{d\rho} \left( \rho \frac{dP}{d\rho} \right) + \frac{1}{\rho^2 \Phi} \frac{d^2 \Phi}{d\phi^2} + k^2 & = \lambda_1 \ . \label{eq:EDO_en_rho_phi}
\end{align}

En este caso, pueden ser soluciones físicas tanto el caso en que $\lambda_1>0$, donde obtendremos soluciones oscilantes (como lo hace Riley), o bien que $\lambda_1 < 0$, donde obtendremos soluciones exponenciales (como lo hace Arfken). La solución que escojamos dependerá de las condiciones de borde del problema.

% la única solución que da sentido físico a nuestro sistema será la oscilante, donde $\lambda_1 < 0$. Esto ocurre debido a que nuestro sistema debe tener condiciones de borde finitas, lo que no ocurre ni en el caso lineal ni en el caso exponencial, donde $Z(z)  \to \infty$ cuando $z \to \infty$. Por ello, y para evitar escribir raíces en nuestras soluciones, podemos establecer que $\lambda_1 = -\ell^2$, con $\ell$ un valor real positivo. 

Ahora, definiendo un valor auxiliar $\eta^2 = k^2 - \lambda_1$, \emph{no necesariamente entero}, podemos separar la ecuación \eqref{eq:EDO_en_rho_phi} en una parte que dependa de $\rho$ y una que dependa de $\phi$, de modo que luego de multiplicar por $\rho^2$ y manipular un poco los términos, obtenemos la ecuación
\begin{equation}
    \frac{\rho}{P} \frac{d}{d\rho} \left( \rho \frac{dP}{d\rho} \right) + \rho^2 \eta^2 = - \frac{1}{\Phi} \frac{d^2 \Phi}{d\phi^2} \ , 
\end{equation}
e introduciendo una constante de separación $\lambda_2$, obtenemos las siguientes expresiones
\begin{align}
    \frac{1}{\Phi} \frac{d^2 \Phi}{d\phi^2} & = - \lambda_2 \ , \label{eq:EDO_en_Phi} \\
    \frac{\rho}{P} \frac{d}{d\rho} \left( \rho \frac{dP}{d\rho} \right) + \eta^2 \rho^2 & = \lambda_2 \ .
\end{align}

Las soluciones físicas para la EDO de $\phi$ deberán ser periódicas, dado que $\Phi(\phi + 2\pi) = \Phi(\phi)$, con lo que necesitamos que $\lambda_2 > 0$, ya que el signo menos en \eqref{eq:EDO_en_Phi} produce que el lado derecho de la igualdad sea negativo. Llamamos $\lambda_2 = m^2$, donde $m$ es un número entero para satisfacer la condición de periodicidad.

Solo nos queda resolver la ecuación radial, es decir, la EDO
\begin{equation}
    \rho \frac{d}{d\rho} \left( \rho \frac{dP}{d\rho} \right) + (\eta^2 \rho^2 - m^2)P = 0 \ ,
\end{equation}
que al realizar el cambio de variable $x = \eta\rho$, $dx = \eta d\rho$, y definiendo $Y(x) = P(\rho) = P(x/\eta)$, puede reescribirse como
\begin{equation}
    x \frac{d}{dx}\left( x \frac{dY}{dx} \right) + (x^2 - m^2)Y = 0 \ ,
\end{equation}
ecuación que se conoce como \textbf{ecuación diferencial de Bessel}, cuyas soluciones son las \emph{funciones de Bessel} que estudiaremos más adelante en el curso.

De esta forma, una solución particular de la ecuación de Helmholtz en coordenadas cilíndricas será
\begin{equation} \label{eq:solucion_particular_cilindricas}
    \psi_{\lambda_1, m}(\rho, \phi, z) = P_{\lambda_1 m}(\rho) \Phi_m(\phi) Z_{\lambda_1}(z) \ ,
\end{equation}
y la solución general será la combinación lineal de las soluciones \eqref{eq:solucion_particular_cilindricas}, con coeficientes a determinar mediante las condiciones de contorno,
\begin{equation}
    \psi(\rho, \phi, z) = \sum_{\lambda_1, m} C_{\lambda_1, m} P_{\lambda_1 m}(\rho) \Phi_m(\phi) Z_{\lambda_1}(z) \ .
\end{equation}

Algo interesante que señalar es que esta solución se mantiene incluso si $k^2$ no es una constante, sino una función de la forma
\begin{equation}
    k^2 \to f(\rho) + \frac{g(\phi)}{\rho^2} + h(z) \ .
\end{equation}

\subsection{Coordenadas esféricas}

En un sistema esférico $(r, \theta,\phi)$ el operador laplaciano se define como
\begin{equation}
    \nabla^2 = \frac{1}{r^2 \sin\theta} \left( \sin\theta \frac{\partial}{\partial r}\left( r^2 \frac{\partial}{\partial r} \right) + \frac{\partial}{\partial \theta} \left( \sin\theta \frac{\partial}{\partial \theta} \right) + \frac{1}{\sin\theta} \frac{\partial^2}{\partial \phi^2} \right) \ ,
\end{equation}
con lo que la ecuación de Helmholtz queda como
\begin{equation} \label{eq:Helmholtz_esferica}
    \frac{1}{r^2 \sin\theta} \left( \sin\theta \frac{\partial}{\partial r}\left( r^2 \frac{\partial \psi}{\partial r} \right) + \frac{\partial}{\partial \theta} \left( \sin\theta \frac{\partial \psi}{\partial \theta} \right) + \frac{1}{\sin\theta} \frac{\partial^2 \psi}{\partial \phi^2} \right) + k^2\psi = 0 \ ,
\end{equation}
por lo que planteamos un \emph{ansatz} de la forma
\begin{equation}
    \psi(r,\theta, \phi) = R(r) \Theta(\theta) \Phi(\phi) \ ,
\end{equation}
que al ser sustituído en la ecuación \eqref{eq:Helmholtz_esferica} resulta en
\begin{equation}
    \frac{\Theta \Phi}{r^2} \frac{d}{dr}\left( r^2 \frac{dR}{dr} \right) + \frac{R\Phi}{r^2\sin\theta} \frac{d}{d \theta} \left( \sin\theta \frac{d\Theta}{d \theta} \right) + \frac{R\Theta}{r^2\sin^2\theta} \frac{d^2\Phi}{d \phi^2} + k^2 R \Theta \Phi = 0 \ .
\end{equation}

Dividiendo por $R\Theta\Phi$ y reordenando los términos, podemos llegar a la expresión
\begin{equation}
    \frac{1}{\Phi} \frac{d^2\Phi}{d\phi^2} = r^2\sin^2\theta \left[ -\frac{1}{Rr^2} \frac{d}{dr}\left( r^2 \frac{dR}{dr} \right) - \frac{1}{\Theta r^2\sin\theta} \frac{d}{d\Theta} \left( \sin\theta \frac{d\theta}{d\theta} \right) - k^2 \right] \ .
\end{equation}

En este momento, podemos separar ambos lados de la ecuación al igualarlas a una constante $\lambda_1$, de modo que obtenemos el sistema de ecuaciones
\begin{align}
    \frac{1}{\Phi} \frac{d^2\Phi}{d\phi^2} & = \lambda_1 \\
    r^2\sin^2\theta \left[ -\frac{1}{Rr^2} \frac{d}{dr}\left( r^2 \frac{dR}{dr} \right) - \frac{1}{\Theta r^2\sin\theta} \frac{d}{d\theta} \left( \sin\theta \frac{d\Theta}{d\theta} \right) - k^2 \right] & = \lambda_1 \ , \label{eq:EDO_theta_phi}
\end{align}
y como en la coordenada azimutal $\phi$ debe cumplirse la condición de periodicidad $\Phi(\phi) = \Phi(\phi + 2\pi)$, entonces $\lambda_1<0$, y lo denotamos por $\lambda_1 = -m^2$, con $m$ un número entero. Luego, reemplazamos este valor en la ecuación \eqref{eq:EDO_theta_phi} para poder separarla. Multiplicando por $r^2$, y manipulando un poco los términos, obtenemos la expresión
\begin{equation}
    \frac{1}{R} \frac{d}{dr}\left( r^2 \frac{dR}{dr} \right) + k^2r^2 = - \frac{1}{\Theta\sin\theta} \frac{d}{d\theta}\left( \sin\theta \frac{d\Theta}{d\theta}\right) + \frac{m^2}{\sin^2\theta} \ .
\end{equation}

Introduciendo una segunda constante de separación $\lambda_2$, obtenemos las ecuaciones
\begin{align}
    \frac{d}{dr}\left( r^2 \frac{dR}{dr} \right) + (k^2r^2 - \lambda_2) R = 0 \\
    \frac{1}{\sin\theta} \frac{d}{d\theta}\left( \sin\theta \frac{d\Theta}{d\theta} \right) + \left( \lambda_2 - \frac{m^2}{\sin^2\theta} \right)\Theta = 0 \ .
\end{align}

En este caso, vale la pena hacer un análisis en función del valor de $m^2$ para la ecuación axial.

\begin{itemize}
    \item Si $m=0$, hacemos el cambio de variable $x = \cos\theta$, de modo que $dx = -\sin \theta d\theta$. Definiendo una función $Y(x) = \Theta(\theta) = \Theta(\arccos x)$, la EDO toma la forma
    \begin{equation}
        \frac{d}{dx}\left[ (1-x^2) \frac{dY}{dx} \right] + \lambda_2 Y = 0 \ ,
    \end{equation}
    lo que corresponde a la \textbf{ecuación diferencial de Legendre}, cuyas soluciones son los \emph{polinomios de Legendre}, que se estudiarán en detalle más adelante en el curso. 
    
    Dado que nuestra variable $x$ es definida a partir de un coseno, esta estará acotada en su dominio, es decir, si $\theta \in [0, \pi]$, entonces $x \in [-1,1]$. Si además exigimos que la solución sea acotada, podemos demostrar (como se hará más adelante) que $\lambda_2 = \ell(\ell+1)$, con $\ell = 0, 1, 2, \dots$.

    \item Si $m \neq 0$, podemos realizar el mismo cambio de variables, obteniendo la ecuación
    \begin{equation}
        \frac{d}{dx}\left[ (1-x^2) \frac{dY}{dx} \right] +  \left( \lambda_2 - \frac{m^2}{1-x^2} \right)Y = 0 \ ,
    \end{equation}
    que corresponde a la \textbf{ecuación diferencial asociada de Legendre}, cuyas soluciones son los \emph{polinomios asociados de Legendre}. Nuevamente, una solución acotada requerirá que $\lambda_2 = \ell(\ell+1)$, pero en este caso no puede tomar cualquier valor entero, sino que debe satisfacer $\ell \geq m$.
\end{itemize}

Por otra parte, en la ecuación radial podemos hacer el cambio de variable $x = kr$, de modo que $dx = k dr$. Definiendo una función $Y(x) = R(r) = R(x/k)$, podemos reescribir la EDO como
\begin{equation}
    \frac{d}{dx}\left( x^2 \frac{dY}{dx} \right) + (x^2 - \lambda_2)Y = 0 \ ,
\end{equation}
que corresponde a la \textbf{ecuación diferencial esférica de Bessel}, cuyas soluciones son las \emph{funciones esféricas de Bessel}, que serán estudiadas más adelante.

En el caso particular en que $k=0$, recuperamos la ecuación de Laplace, y la ecuación radial se convierte en una EDO de Euler-Cauchy,
\begin{equation}
    r^2 R'' + 2r R' - \lambda_2 R = 0 \ ,
\end{equation}
con soluciones que dependen del valor de $\lambda_2$. Sin embargo, al exigir soluciones finitas e imponer que $\lambda_2 = \ell(\ell+1)$, solo tenemos una solución posible, de la forma
\begin{equation}
    R(r) = A r^\ell + B r^{-(\ell+1)} \ .
\end{equation}

Así, una solución particular será dada por
\begin{equation}
    \psi_{\ell, m}(r, \theta, \phi) = R_\ell(r) \Phi_m(\phi) \Theta_{\ell, m}(\theta) \ ,
\end{equation}
y la solución general corresponde a su combinación lineal con coeficientes a determinar mediante las condiciones de contorno,
\begin{equation}
    \psi(r, \theta, \phi) = \sum_{\ell, m} R_\ell(r) \Phi_m(\phi) \Theta_{\ell, m}(\theta) \ .
\end{equation}

\section{El método de series para EDO}

Hasta ahora, hemos estudiado ecuaciones que al cumplir ciertas propiedades, como tener una forma u otra, sabemos hallar una solución de una forma sencilla, como una ecuación de Euler-Cauchy, una EDO lineal de segundo orden con coeficientes constantes, entre otras. Sin embargo, existen algunas ecuaciones que pese a aparentar ser sencillas, no es posible hallar una solución en términos de funciones conocidas, como es el caso de la \emph{ecuación de Airy}
\begin{equation}
    y'' - xy = 0 \ .
\end{equation}
En estos casos, podemos hacer uso del llamado \emph{método de series de potencias} para obtener una solución que, si bien no tiene una forma compacta, nos permite resolver dichos problemas, y encontrar aproximaciones válidas.


% \section{Resolver EDOs por el método de series}

Consideremos una ecuación diferencial ordinaria, lineal y de segundo orden cuyos coeficientes son polinomios\footnote{En este caso, por \emph{polinomios} nos referimos a términos de la forma $(x-x_0)^k$, donde $k$ puede tomar tanto valores positivos como negativos.} en la variable independiente del problema, de la forma
\begin{equation} \label{eq:EDO_Series}
    \frac{d^2 y}{dx} + P(x) \frac{dy}{dx} + Q(x) y(x) = 0 \ .
\end{equation}

En general, consideraremos que $P(x)$ y $Q(x)$ son funciones \emph{analíticas} de $x$ en, al menos, un punto $x=x_0$ a menos que se indique lo contrario.

Antes de discutir la existencia de soluciones para esta ecuación, debemos definir las nociones de punto \emph{ordinario} y de punto \emph{singular}.

\begin{defi} \marginnote{Puntos ordinarios y singulares}
    Dada una ecuación diferencial de la forma \eqref{eq:EDO_Series}, decimos que un punto $x_0$ es un \textbf{punto ordinario} de la ecuación si las funciones $P(x)$ y $Q(x)$ son \emph{analíticas} en $x=x_0$. En caso de que $P(x)$ o $Q(x)$ diverjan en $x = x_0$, diremos que $x_0$ es un \textbf{punto singular}. 
\end{defi}

\begin{defi} \marginnote{Puntos singulares regulares y esenciales}
    Dado un punto singular $x=x_0$ para una EDO de la forma \eqref{eq:EDO_Series}, diremos que $x_0$ es un \textbf{punto singular regular} si tanto $(x-x_0)P(x)$ como $(x-x_0)^2Q(x)$ son analíticas en $x=x_0$. En caso contrario, $x_0$ es un \textbf{punto singular irregular, o esencial}.
\end{defi}

% Cuando nuestra ecuación de la forma \eqref{eq:EDO_Series} tiene únicamente puntos ordinarios, podemos utilizar el llamado \emph{método de series} para resolverla. Para ello, seguiremos el algoritmo presentado a continuación:
\begin{propo} \marginnote{Método de Series}
    \textbf{Método de Series}

    Dada una EDO lineal de segundo orden de la forma \eqref{eq:EDO_Series} sin singularidades en su dominio de definición, es decir, donde únicamente existen puntos ordinarios, podemos proponer una solución en serie de potencias alrededor del punto $x_0$, de la forma
    \begin{equation}
        y(x) = \sum_{n=0}^\infty a_n(x-x_0)^n \ , \quad a_0 \neq 0 \ .
    \end{equation}
    Para ello, seguiremos este algoritmo, asumiendo $x_0 = 0$ para no sobrecargar la notación:
    \begin{enumerate}
        \item Hallamos la primera y segunda derivada de nuestra serie, re-indexando, de ser necesario, de modo que ambas queden en términos de $x^n$:
        \begin{align}
            y'(x) & = \sum_{k=1}^\infty k a_k x^{k-1} \nonumber \\
            & = \sum_{k=0}^\infty (k+1) a_{k+1} x^{k} \\
            y''(x) & = \sum_{k=1}^\infty k (k+1) a_{k+1} x^{k-1} \nonumber \\
            & = \sum_{k=0}^\infty (k+2) (k+1) a_{k+2} x^{k} \ .
        \end{align}
        \item Sustituímos las derivadas en nuestra EDO homogénea \eqref{eq:EDO_Series}, obteniendo la expresión
        \begin{equation*}
            \sum_{k=0}^\infty [ (k+1)(k+2)a_{k+2} + P(x) (k+1)a_{k+1} + Q(x) a_k ] x^k = 0 \ .
        \end{equation*}
        \item Dado que buscamos soluciones no triviales, la igualdad anterior se satisface si, término a término, se cumple la relación de recurrencia
        \begin{equation}
            a_{k+2} = -\frac{P(x) (k+1)a_{k+1} + Q(x) a_k}{(k+1)(k+2)} \ .
        \end{equation}
        \item Podemos determinar los primeros coeficientes, $a_0$ y $a_1$, a partir de las condiciones de contorno del sistema, por lo que, en general, actuarán como los coeficientes de dos soluciones linealmente independientes para nuestro problema.
    \end{enumerate}
\end{propo}

\begin{ejemplo}
    Encuentre una solución a la ecuación de Airy
    \begin{equation}
        y'' - xy = 0 \ ,
    \end{equation}
    utilizando el método de series alrededor de $x_0 = 0$.

    \textbf{Solución.} Según la estructura \eqref{eq:EDO_Series}, tenemos que $P(x) = 0$ y $Q(x) = -x$. Como $P(x)$ y $Q(x)$ son funciones analíticas en $\mathbb{R}$ (función constante y lineal, respectivamente) y en particular en $x_0 = 0$, entonces $x_0 = 0$ es un punto ordinario de la EDO, por lo que podemos utilizar el método de series. Entonces, postulamos una solución de la forma
    \begin{equation}
        y(x) = \sum_{n=0}^\infty a_n x^n \ .
    \end{equation}

    Calculando las derivadas, tenemos
    \begin{align}
        y'(x)  & = \sum_{n=1}^\infty na_n x^{n-1} = \sum_{n=0}^\infty (n+1) a_{n+1} x^n \ , \\
        y''(x) & = \sum_{n=2}^\infty n(n-1) a_n x^{n-2} = \sum_{n=0}^\infty (n+2) (n+1) a_{n+2} x^n \ .
    \end{align}

    Reemplazamos en nuestra ecuación original, obteniendo la expresión 
    \begin{align}
        \sum_{n=0}^\infty (n+2) (n+1) a_{n+2} x^n - x \sum_{n=0}^\infty a_n x^n & = 0 \\
        \sum_{n=0}^\infty (n+2) (n+1) a_{n+2} x^n - \sum_{n=0}^\infty a_n x^{n+1} & = 0 \\
        2a_2 + \sum_{n=1}^\infty (n+2) (n+1) a_{n+2} x^n - \sum_{n=1}^\infty a_{n-1} x^{n} & = 0 \ .
    \end{align}

    Igualamos la expresión a cero término a término, ya que buscamos una solución no trivial. Esto conlleva a que $a_2 = 0$, y a la relación de recurrencia
    \begin{align}
        (n+2)(n+1) a_{n+2} & = a_{n-1} \ , \\
        a_{n+2} & = \frac{a_{n-1}}{(n+1)(n+2)} \ .
    \end{align}

    Desarrollemos esta relación de recurrencia. Notamos que
    \begin{align*}
        n = 1; & \quad a_3 = \frac{a_0}{2 \cdot 3} \\
        n = 2; & \quad a_4 = \frac{a_1}{3 \cdot 4} \\
        n = 3; & \quad a_5 = \frac{a_2}{4 \cdot 5} = 0 \\
        n = 4; & \quad a_6 = \frac{a_3}{5 \cdot 6} = \frac{a_0}{2 \cdot 3 \cdot 5 \cdot 6} \\
        n = 5; & \quad a_7 = \frac{a_4}{6 \cdot 7} = \frac{a_1}{3 \cdot 4 \cdot 6 \cdot 7} \\
        n = 6; & \quad a_8 = \frac{a_5}{7 \cdot 8} = 0 \\
        n = 7; & \quad a_9 = \frac{a_6}{8 \cdot 9} = \frac{a_0}{2 \cdot 3 \cdot 5 \cdot 6 \cdot 8 \cdot 9} \ ,
    \end{align*}
    y así sucesivamente. Por lo tanto, tenemos que
    \begin{align}
        y(x) & = a_0 + a_1x + a_2 x^2 + a_3 x^3 + \dots \\
        & = a_0 \left( 1 + \frac{x^3}{2 \cdot 3} + \frac{x^6}{2 \cdot 3 \cdot 5 \cdot 6} + \frac{x^9}{2 \cdot 3 \cdot 5 \dot 6 \cdot 8 \cdot 9} + \dots \right) \\
        & \qquad + a_1 \left( x + \frac{x^4}{3 \cdot 4} + \frac{x^7}{3 \cdot 4 \cdot 6 \cdot 7} + \frac{x^{10}}{3 \cdot 4 \cdot 6 \cdot 7 \cdot 9 \cdot 10} + \dots  \right) \\
        & = a_0 \left( 1 + \sum_{n=1}^\infty \frac{x^{3n}}{2 \cdot 3 \cdot 5 \cdot 6 \cdot \dots \cdot (3n-1) \cdot (3n)} \right) \\
        & \qquad + a_1 \left( x + \sum_{n=1}^\infty \frac{x^{3n+1}}{3 \cdot 4 \cdot 6 \cdot 7 \cdot \dots \cdot (3n) \cdot (3n+1)}  \right) \\
        & = a_0 \operatorname{Ai}(x) + a_1 \operatorname{Bi}(x) \ ,
    \end{align}
    donde $\operatorname{Ai}$ y $\operatorname{Bi}$ son las \emph{funciones de Airy}, que son linealmente independientes entre sí.
\end{ejemplo}

\begin{ejemplo}
    Considere la ecuación de Hermite
    \begin{equation}
        y'' - 2x y' + \lambda y = 0 \ , \qquad x \in \mathbb{R} \ ,
    \end{equation}
    donde $\lambda$ es un valor constante. Encuentre la solución general en serie de potencias.

    \textbf{Solución.} Según la estructura \eqref{eq:EDO_Series}, tenemos que $P(x) = -2x$ y $Q(x) = \lambda$. Como $P(x)$ y $Q(x)$ son funciones analíticas en $\mathbb{R}$ (función lineal y constante, respectivamente) y en particular en $x_0 = 0$. Entonces, $x_0 = 0$ es un punto ordinario de la EDO, por lo que podemos utilizar el método de series. Entonces, postulamos una solución de la forma
    \begin{equation}
        y(x) = \sum_{n=0}^\infty a_n x^n \ .
    \end{equation}

    Reemplazamos en la ecuación original, obteniendo
    \begin{align}
        \sum_{n=0}^\infty (n+2)(n+1)a_{n+2}x^n - 2x \sum_{n=0}^\infty (n+1) a_{n+1}x^n + \lambda \sum_{n=0}^\infty a_n x^n & = 0 \\
        \sum_{n=0}^\infty (n+2)(n+1)a_{n+2}x^n - 2 \sum_{n=0}^\infty (n+1) a_{n+1}x^n+1 + \lambda \sum_{n=0}^\infty a_n x^n & = 0 \\
        % 2 a_2 + \sum_{n=1}^\infty (n+2)(n+1)a_{n+2}x^n - \sum_{n=1}^\infty 2n a_{n}x^n + \lambda a_0 + \lambda \sum_{n=1}^\infty a_n x^n & = 0 \\
        2 a_2 + \lambda a_0 + \sum_{n=1}^\infty [(n+2)(n+1)a_{n+2} - 2na_n + \lambda a_n]x^n & = 0 \ .
    \end{align}

    Para que la igualdad a cero sea verdadera, descartando la solución trivial ($x=0$), deberán cumplirse las relaciones de recurrencia
    \begin{equation}
        a_2 = - \frac{\lambda}{2}a_0 \ , \qquad a_{n+2} = \frac{2n-\lambda}{(n+1)(n+2)}a_n \ .
    \end{equation}
    Observamos que
    \begin{align*}
        n=1; & \quad a_3 = \frac{2-\lambda}{2 \cdot 3} a_1 \\
        n=2; & \quad a_4 = \frac{4-\lambda}{3 \cdot 4} a_2 = - \frac{(4-\lambda) \lambda}{2 \cdot 3 \cdot 4} a_0 \\
        n=3; & \quad a_5 = \frac{6-\lambda}{4 \cdot 5} a_3 = \frac{(6-\lambda)(2-\lambda)}{2 \cdot 3 \cdot 4 \cdot 5} a_1 \\
        n=4; & \quad a_6 = \frac{8-\lambda}{5 \cdot 6}a_4 = - \frac{(8-\lambda)(4-\lambda)\lambda}{2 \cdot 3 \cdot 4 \cdot 5 \cdot 6} a_0 \\
        n=5; & \quad a_7 = \frac{10-\lambda}{6 \cdot 7} a_5 = \frac{(10-\lambda)(6-\lambda)(2-\lambda)}{2 \cdot 3 \cdot 4 \cdot 5 \cdot 6 \cdot 7} a_1 \ ,
    \end{align*}
    y así sucesivamente. Luego, 
    \begin{align}
        y(x) & = a_0 + a_1x + a_2 x^2 + a_3 x^3 + \dots \nonumber \\
        & = a_0 \left( 1 - \frac{\lambda}{2}x^2 - \frac{(4-\lambda)\lambda}{2 \cdot 3 \cdot 4}x^4 - \frac{(8-\lambda)(4-\lambda)\lambda}{2 \cdot 3 \cdot 4 \cdot 5 \cdot 6}x^6 + \dots \right) \nonumber \\
        & \qquad + a_1 \left( x + \frac{2-\lambda}{2 \cdot 3}x^3 + \frac{(6-\lambda)(2-\lambda)}{2 \cdot 3 \cdot 4 \cdot 5} x^5 + \frac{(10-\lambda)(6-\lambda)(2-\lambda)}{2 \cdot 3 \cdot 4 \cdot 5 \cdot 6 \cdot 7}x^7 + \dots \right) \nonumber \\
        & = a_0 \left( 1 - \sum_{n=1}^\infty \frac{\lambda(4-\lambda) \dots (4n - 4 - \lambda)}{(2n)!} x^{2n} \right) \nonumber \\
        & \qquad + a_1 \left( x + \sum_{n=1}^\infty \frac{(2-\lambda)(6-\lambda)\dots (4n-2-\lambda)}{(2n+1)!} x^{2n+1} \right) \ .
    \end{align}

    En el caso en que $\lambda$ sea un número par no negativo, una de las dos series se hace finita. Es más, cuando $\lambda = 2n$, y normalizando por una constante adecuada, obtenemos los \emph{polinomios de Hermite} $H_n(x)$, que surgen como soluciones a la ecuación de Schrödinger para un oscilador armónico cuántico.
\end{ejemplo}

En el caso en que el coeficiente $P(x)$ no se anula en el dominio del sistema, podemos garantizar la existencia de una solución para \eqref{eq:EDO_Series} mediante el siguiente teorema\footnote{En este apunte hemos omitido gran parte de la discusión sobre la naturaleza de este tipo de soluciones. Si desea revisarla en más detalle, puede consultar el capítulo 3, sección 5 de \cite{Butkov}.}
\begin{teorema}[de Fuchs]\label{teo:Fuchs}
    Una ecuación diferencial de la forma \eqref{eq:EDO_Series} posee al menos una solución expresable en una serie de potencias en torno a $x=x_0$ si este es un punto ordinario o un punto singular regular de la EDO \eqref{eq:EDO_Series}. La solución será de la forma 
    \begin{equation} \label{eq:solucion_serie}
        y(x) = \sum_{n=0}^\infty a_n(x-x_0)^{n+s} \ , \qquad a_0 \neq 0
    \end{equation}
    para un número $s$ real y positivo (si $x_0$ es un punto ordinario, entonces $s=0$), o bien, de la forma 
    \begin{equation}\label{eq:solucion_logaritmo}
        y(x) = y_0 \ln(x-x_0) + \sum_{n=0}^\infty b_n (x-x_0)^{n+r} \ , \qquad b_0 \neq 0 
    \end{equation}
    para algún número $r$ real y positivo, donde $y_0$ es una solución de la forma \eqref{eq:solucion_serie}.

    A la solución de la forma \eqref{eq:solucion_serie} se le conoce como \textbf{serie de Frobenius}, mientras que a una solución de la forma \eqref{eq:solucion_logaritmo} se le suele denominar \textbf{serie generalizada de Frobenius}.
\end{teorema}

\begin{propo} \marginnote{Método de Frobenius}
    \textbf{Método de Frobenius.}

    Dada una EDO lineal de segundo orden de la forma \eqref{eq:EDO_Series}, podemos proponer una solución en forma de serie de Frobenius \eqref{eq:solucion_serie}, que en general, podrá ser diferenciada al menos dos veces dentro de su radio de convergencia. Podremos determinar los coeficientes de nuestra serie siguiendo el siguiente algoritmo, donde hemos asumido $x_0 = 0$ para no sobrecargar la notación:
    \begin{enumerate}
        \item Hallamos la primera y segunda derivada de nuestra serie, re-indexando, de ser necesario, de modo que siempre empecemos nuestra suma en $k=0$. Esto es,
        \begin{align}
            y'(x)  & = \sum_{k=0}^\infty (k+s) A_k x^{k+s-1} 
            % \nonumber
            \ ,
            \\
            % & = sA_0 x^{s-1} + \sum_{k=1}^\infty (k+s) A_k x^{k+s-1} \nonumber \\
            % & = sA_0 x^{s-1} + \sum_{k=0}^\infty (k+s+1) A_{k+1} x^{k+s} \\
            y''(x) & = \sum_{k=0}^\infty (k+s-1)(k+s) A_k x^{k+s-2} \ .
            % \nonumber \\
            % & = s(s-1)A_0 x^{s-2} + \sum_{k=1}^\infty (k+s-1)(k+s) A_k x^{k+s-2} \nonumber \\
            % & = s(s-1)A_0 x^{s-2} + \sum_{k=0}^\infty (k+s)(k+s+1) A_{k+1} x^{k+s-1} 
        \end{align}
        \item Sustituimos las derivadas en nuestra EDO homogénea \eqref{eq:EDO_Series}. Multiplicamos la ecuación por $x^2$ (en general, por $(x-x_0)^2$), y luego de manipulaciones algebraicas, obtenemos la expresión
        \begin{equation}\label{eq:metodo_frobenius_EDO}
            [s(s-1) + p(x)s + q(x)]A_0 x^s + \sum_{k=1}^\infty [(k+s-1)(k+s) + p(x)(k+s) + q(x)]A_k x^{k+s} = 0 \ ,
        \end{equation}
        donde $p(x) = xP(x)$ y $q(x) = x^2 Q(x)$.\footnotemark
        \item Dado que la ecuación \eqref{eq:metodo_frobenius_EDO} es nula término a término y asumiendo que el coeficiente $A_0$ es no nulo, podemos obtener la \emph{ecuación indicial} para $k=0$,
        \begin{equation} \label{eq:ecuacion_indicial}
            I(s) = s(s-1) + p(0)s + q(0) \ .
        \end{equation}
        \item Igualando a cero las potencias superiores de $x$, nos lleva a las \emph{relaciones de recurrencia} de la forma
        \begin{equation}
            A_k = f(j,s) A_j \ ,
        \end{equation}
        donde $j$ es un índice tal que $j > k$. Usualmente, podemos hacer que estas dependan de solo uno o dos coeficientes, típicamente $A_0$ y $A_1$.
        \item A partir de nuestras condiciones de contorno, podemos determinar los valores de los coeficientes $A_k$. Además, es posible determinar el valor de $s$ a partir de la ecuación indicial \eqref{eq:ecuacion_indicial}.
        \item La solución general a nuestra ecuación dependerá de las raíces $s_1$ y $s_2$ de la ecuación indicial:
        \begin{itemize}
            \item[a.] Si $s_1$ y $s_2$ son tales que su diferencia \emph{no es un número entero}, entonces la solución general es de la forma
            \begin{align}
                y(x) & = C_1 y_1(x) + C_2 y_2(x) \nonumber \\
                & = C_1 \left( x^{s_1} \sum_{k=0}^\infty a_k x^k \right) + C_2 \left( x^{s_2} \sum_{k=0}^\infty a_k x^k \right)  \ .
            \end{align}
            \item[b.] Si $s_1 = s_2$, entonces el método de Frobenius nos entrega solo una solución, y la segunda solución será una serie de Frobenius generalizada \eqref{eq:solucion_logaritmo}, de modo que la solución general será
            \begin{align}
                y(x) & = C_1 y_1(x) + C_2 y_2(x) \nonumber \\
                & = C_1 \left( x^{s} \sum_{k=0}^\infty a_k x^k \right) + C_2 \left( \alpha y_1(x) \ln(x) + \sum_{k=0}^\infty b_n x^{k+s}  \right) \ ,
            \end{align}
            donde $\alpha$ es una constante a determinar.
            \item[c.] Si $s_1$ y $s_2$ son tales que su diferencia \emph{sí es un número entero}, existirá al menos una solución en forma de serie de potencias, correspondiente a la raíz mayor, mientras que la segunda puede ser tanto una serie de potencias (a.) o una solución logarítmica (b.).  
        \end{itemize}
    \end{enumerate}
\end{propo}
\footnotetext{{Puede encontrar algo más de información respecto a esta elección en el capítulo 6 de \cite{Zill_2013}. Para efectos de este curso, confíen en que funciona.}}

% \begin{teorema}
%     Si $x=x_0$ es un punto ordinario de la ecuación \eqref{eq:EDO_Series}, entonces podemos hallar la solución general a partir de una serie de potencias mediante el método de Frobenius con $s=0$, con ciertas modificaciones, las que son:
%     \begin{enumerate}
%         \item Nos interesa que las derivadas puedan expresarse en términos de $x^n$, re-indexando de ser necesario.
%         \item No multiplicamos la ecuación por $x^2$.
%         \item Omitimos este paso.
%     \end{enumerate}
    
%     Esto dará origen a dos soluciones, $y_1$ e $y_2$, las que son linealmente independientes.
% \end{teorema}


